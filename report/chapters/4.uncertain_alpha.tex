\chapter{Optimization with uncertain \texorpdfstring{$\alpha$}{alpha} functions}
\label{chap:unc_alpha}

\section{Contextual hypotesis}

\subsection{Scenario}

We now assume that the binary features of the users cannot be observed and therefore data is considered as \textbf{aggregated}.

Since the features of the users are \textbf{not observable}, the $\alpha$ functions' shape for each class is unknown.

As a result, in our scenario the learner receives all the interactions minus the parameters of the $\alpha$ functions.

\section{Algorithm}

\subsection{Solving the problem}

By gathering the aggregated reward for each product we are able to utilize those coarse rewards to generate feedbacks for our \textbf{MABs} and therefore train them on the aggregated interactions for each day.

In particular we exploit \textbf{Gaussian Processes} in conjunction with \textbf{MAB} algorithms such as \textbf{Thompson Sampling} and \textbf{UCB1} to exploit the continuity between the different arms.

We instantiate a \textbf{GP-MAB} for each subcampaign and each \textbf{MAB} will have \texttt{n\_budget\_steps} number of arms.

\subsection{Algorithms outline}

\textbf{GPTS} is a variant of \textbf{TS} implemented using \textbf{Gaussian Processes}:

\begin{itemize}
	\item For each day $t$, we gather a sample from each arm $a$:
		\begin{displaymath}
			\tilde{\theta_a} \leftarrow \text{ Sample} \left( \mathbb{P}(\mu_a = \theta_a) \right)
		\end{displaymath}
	\item Play arm $a_t$ defined as:
		\begin{displaymath}
			a_t \leftarrow arg\max_{a \in A} \left\{ \tilde{\theta_a} \right\}
		\end{displaymath}
	\item Update the \textbf{Gaussian Process} with the reward obtained.
\end{itemize}

\textbf{GPUCB1} is a variant of \textbf{UCB1} that takes advantage of the \textbf{Gaussian Processes} confidence interval and models it as the confidence bound; apart from the arm choice, the learning process is equal to \textbf{GPTS}.

\begin{displaymath}
	a_t \leftarrow arg\max_{a \in A} \left\{ \mu_{t-1} + \delta \sigma_{t-1} \right\}
\end{displaymath}

Code snippet that calculates the \textbf{upper bound}:

\begin{lstlisting}[style=Python, basicstyle=\tiny, numbers=none, xrightmargin=15px]
def estimation(self):
	upper_bounds = (self.means + self.confidence * 1.96 * self.sigmas)
			* self.normalize_factor
	return upper_bounds
\end{lstlisting}

\section{Results}

\subsection{Single run reward and regret}

Thompson Sampling and UCB

\begin{center}
	\includegraphics[scale=0.4]{img/Graphs/uncertain_alpha/image1.png}
	\includegraphics[scale=0.4]{img/Graphs/uncertain_alpha/image2.png}
\end{center}

Regret comparison

\begin{center}
	\includegraphics[scale=0.5]{img/Graphs/uncertain_alpha/image3.png}
\end{center}

\subsection{Average regret and reward}

Thompson Sampling and UCB

\begin{center}
	\includegraphics[scale=0.4]{img/Graphs/uncertain_alpha/image4.png}
	\includegraphics[scale=0.4]{img/Graphs/uncertain_alpha/image5.png}
\end{center}

Average regret comparison

\begin{center}
	\includegraphics[scale=0.45]{img/Graphs/uncertain_alpha/image6.png}
\end{center}

\begin{displaymath}
	\text{Regret ratio } = \frac{\text{Avg regret}}{\text{Upper bound}} = \frac{31019.40}{?} = ?
\end{displaymath}

{\scriptsize Values reference GPTS regret compared to advertising GP regret found at [SLIDE REFERENCE]}

\todo{complete}

\subsection{Conclusions}

Overall we can observe more instability in the \textbf{TS} algorithm, but a faster convergence w.r.t to the \textbf{UCB} approach.

Both algorithms clearly converge to the optimal solution at different rates while respecting a linear cumulative regret bound.

Average results over 15 runs at time horizon $T = 50$:

\begin{table}[h]
	\begin{tabular}{|c|cc|c|}
	\hline \hline
		\cellcolor{blue!25} & Reward 	& Regret	& Deviation \\
	\cline{2-4}
		\cellcolor{blue!25} & $\mu$		& $\mu$		& $\sigma$	\\
	\hline \hline
		GPTS 				& 11610.53 	& 31024.67	& 440.36 	\\
	\hline
		GPUCB				& 11737.73	& 43686.67	& 349.47	\\
	\hline \hline
	\end{tabular}
\end{table}

\todo{adapt from import}
