% ----------------------------------------

\subsection{General Approach}

% ----------------------------------------

\todo{This entire subsection will be relocated in the introduction after merging}

\begin{frame}

\frametitle{Learner interface}
\framesubtitle{Description}

In order for our project to have a more general outline, we decided to model a \textbf{generic learner interface} to standardize how our agents should be expected to behave while learning the budget distribution for a set of subcampaigns in a specific scenario defined by the environment.

In particular, each learner is characterized by the \textit{learn} and \textit{predict} actions.
Every different type of learner will also receive a customized set of information filtered by the \textit{masked environment} (in line with each project step) and potentially employs different algorithms to learn and predict the various budgets.

\end{frame}

% ----------------------------------------

\begin{frame}[fragile]

\frametitle{Learner interface}
\framesubtitle{Code}

%Arguments: interactions: the interactions of the users which led to the given reward reward: the reward obtained from the environment based on the prediction given, needed for the tuning of internal properties done by the learner
%prediction: array containing the previous budget evaluation of the learner
%
%Arguments: data: up-to-date, complete or incomplete environment information that is used by the learner in order to make the inference
%Returns: a tuple containing a list of values (corresponding to the budgets inferred given the knowledge obtained by the learner until now) and a list of features (referring to which particular customers were the budgets aimed for, if None, the budgets apply to all the customers)

\begin{lstlisting}[style=Python, basicstyle=\tiny, numbers=none, framexrightmargin=-20pt]
class Learner(ABC):

  # Updates the learner's properties according to the reward received.
  \@@abstractmethod@@/
  def learn(self, interactions: List[Interaction], reward: float,
            prediction: np.ndarray):
    pass

  # Makes an inference about the values of the budgets for the subcampaigns
  # from the information got over time and the current environment
  \@@abstractmethod@@/
  def predict(self, data: MaskedEnvironmentData) ->
              Tuple[np.ndarray, Optional[List[List[Feature]]]]:
    pass

  # Creates a figure and plots showing the status of learning progress
  \@@abstractmethod@@/
  def show_progress(self, fig: plt.Figure):
    pass

\end{lstlisting}

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{Comparing results}

For each different learner, when possible, we will show our results through the comparison of the rewards obtained by each algorithm against the rewards achieved (in the same enviroment setting) from the \textbf{Clairvoyant learner} and the Stupid learner.
This, aside from theoretical formulations, will help us to define \textbf{upper bounds} and \textbf{lower bounds} for different solutions.

In particular, the \textbf{Clairvoyant learner} makes its "prediction" with full knowledge of the problem while the stupid learner always subdivides \textit{equally} the budget between products.

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{Comparing results}
\framesubtitle{Clairvoyant learner}

\todo{Our implementation for the clairvoyant learner - also mention non determinism and blueprint system}

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{Comparing results}
\framesubtitle{Regret}

\todo{General regret formulation: see slides 1-05 and 1-06}

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{Gaussian processes}

\todo{GP description: see slides 3-05}

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{UCB1 formulation}
\framesubtitle{Outline}

In the \textbf{UCB1} algorithm, every \textit{arm} is associated with an \textbf{upper confidence bound} which provides an \textit{estimation} of the reward gained by playing that specific arm.

When each arm has been played at least once in order to have a baseline for its reward, at every trial, the arm with the \textbf{highest upper confidence bound} is pulled (\textit{optimism in the face of uncertainty}).
After having collected the realization of the reward for the chosen arm, its \textit{upper confidence bound} is updated accordingly.

The main advantage of combining \textbf{gaussian processes} with \textbf{UCB1} is that, in this way it's possible to take advantage of the \textbf{GP}'s \textit{confidence interval} by modeling it as a \textbf{confidence bound}.

\end{frame}

% ----------------------------------------

\begin{frame}[fragile]

\frametitle{UCB1 formulation}
\framesubtitle{Formalities}

Code snippet that calculates the \textbf{upper bound}:

\begin{lstlisting}[style=Python, basicstyle=\tiny, numbers=none, framexrightmargin=-20pt]
def estimation(self):
  upper_bounds = (self.means + self.confidence * 1.96 * self.sigmas)
                   * self.normalize_factor
  return upper_bounds
\end{lstlisting}

The \textit{theoretical regret} given by this kind of algorithm is bounded by:

\begin{displaymath}
R(UCB1) \le \sum_{a:\mu_a < \mu_a^*} \frac{4log(T)}{\Delta_a} + 8\Delta_a
\end{displaymath}

where $\Delta_a$ is the difference in expected reward between the \\ optimal arm $a^*$ and the arm $a$: $\Delta_a = \mu_a^* - \mu_a$

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{TS formulation}
\framesubtitle{Outline}

Some aspects of \textbf{TS} are similar to the \textbf{UCB1} since both are bandit algorithms, the main difference is that in \textbf{TS} every arm is associated to a $prior \beta distribution$.

Every arm has a prior on its \textbf{expected value} based on its mean distribution.
After drawing a sample according to the corresponding prior distribution, the algorithm chooses the arm with the \textbf{best sample}, then, it updates the distribution of the chosen arm according the observed realization.

Both \textbf{Thompson Sampling} (\textbf{TS}) and \textbf{Upper Confidence Bound} (\textbf{UCB}, in particular \textbf{UCB1}), in conjunction with \textbf{Gaussian Processes} are the base algorithms chosen for the learners in our project.

\end{frame}

% ----------------------------------------

\begin{frame}[fragile]

\frametitle{TS formulation}
\framesubtitle{Formalities}

The \textit{theoretical regret} given by this kind of algorithm is bounded by:

\begin{displaymath}
R(TS) \le (1+\epsilon) \sum_{a:\mu_a < \mu_a^*} \frac{\Delta_a(log(T)+log(log(T)))}{K*\Lambda(\mu_a*, \mu_a)} + C (\epsilon, \mu_{a_1} , ... , \mu_{a_{\left|A\right|}})
\end{displaymath}

where $K*\Lambda(mu_a*,mu_a)$ is the \textbf{Kullback-Leibler divergence} and, as before, $\Delta_a$ is defiened as the difference in expected reward between the optimal arm $a^*$ and the arm $a$: $\Delta_a = \mu_a^* - \mu_a$

\end{frame}

% ----------------------------------------

\subsection{Contextual hypotesis}

% ----------------------------------------

\begin{frame}

\frametitle{Scenario}

We now assume that the binary features of the users cannot be observed and therefore data is considered as \textbf{aggregated}.

Since the features of the users are \textbf{not observable}, the $\alpha$ functions' shape for each class is unknown.

As a result, in our scenario the learner receives all the interactions minus the parameters of the $\alpha$ functions.

\end{frame}

% ----------------------------------------

\subsection{Algorithm}

% ----------------------------------------

\begin{frame}

\frametitle{Solving the problem}

\todo{how do we solve the problem}

\end{frame}

% ----------------------------------------

\begin{frame}

\frametitle{Algorithm outline}

Our \textbf{Alphaless Learner} creates 5 different \textbf{GP-MABs} (one for each product) with $n_budget_steps$ number of arms.
They learn and predict on the aggregated budget matrix and try to find the optimal allocation of the budget.

\todo{how the algorithm works}

\end{frame}

% ----------------------------------------

\subsection{Results}

% ----------------------------------------

\begin{frame}

\frametitle{}
\framesubtitle{}

\todo{results}

\end{frame}

% ----------------------------------------
